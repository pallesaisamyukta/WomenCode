{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T19:47:55.300631Z","iopub.status.busy":"2024-04-25T19:47:55.298912Z","iopub.status.idle":"2024-04-25T19:47:55.307492Z","shell.execute_reply":"2024-04-25T19:47:55.306681Z","shell.execute_reply.started":"2024-04-25T19:47:55.300597Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import ast  # Library for parsing strings containing lists\n","import pandas as pd\n","import zipfile\n","import os"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-25T19:47:56.497600Z","iopub.status.busy":"2024-04-25T19:47:56.496799Z","iopub.status.idle":"2024-04-25T19:47:58.918060Z","shell.execute_reply":"2024-04-25T19:47:58.917174Z","shell.execute_reply.started":"2024-04-25T19:47:56.497567Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>Instruction</th>\n","      <th>Response</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>137739</td>\n","      <td>Tags: ['60-minutes-or-less', 'time-to-make', '...</td>\n","      <td>Name: arriba   baked winter squash mexican sty...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>31490</td>\n","      <td>Tags: ['30-minutes-or-less', 'time-to-make', '...</td>\n","      <td>Name: a bit different  breakfast pizza Minutes...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       id                                        Instruction  \\\n","0  137739  Tags: ['60-minutes-or-less', 'time-to-make', '...   \n","1   31490  Tags: ['30-minutes-or-less', 'time-to-make', '...   \n","\n","                                            Response  \n","0  Name: arriba   baked winter squash mexican sty...  \n","1  Name: a bit different  breakfast pizza Minutes...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Read the CSV file with the first column as index\n","df = pd.read_csv('/kaggle/input/inst-resp/inst-resp.csv', index_col=0)\n","df.head(2)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T19:47:59.905174Z","iopub.status.busy":"2024-04-25T19:47:59.904795Z","iopub.status.idle":"2024-04-25T19:47:59.917403Z","shell.execute_reply":"2024-04-25T19:47:59.916427Z","shell.execute_reply.started":"2024-04-25T19:47:59.905142Z"},"trusted":true},"outputs":[],"source":["# Load the dataset\n","inst = df.Instruction.to_list()  # List of instructions strings\n","resp = df.Response.to_list()  # List of responses strings"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T19:48:00.111285Z","iopub.status.busy":"2024-04-25T19:48:00.110997Z","iopub.status.idle":"2024-04-25T19:48:00.126664Z","shell.execute_reply":"2024-04-25T19:48:00.125808Z","shell.execute_reply.started":"2024-04-25T19:48:00.111260Z"},"trusted":true},"outputs":[],"source":["class RecipeDataset(Dataset):\n","    \"\"\"\n","    PyTorch dataset for recipe generation.\n","\n","    Args:\n","        ingredients (list): List of ingredients for input.\n","        recipes (list): List of recipes for output.\n","        tokenizer (transformers.Tokenizer): Tokenizer for encoding text.\n","        max_length (int): Maximum length of input and output sequences.\n","    \"\"\"\n","    \n","    def __init__(self, ingredients, recipes, tokenizer, max_length=512):\n","        \"\"\"\n","        Initialize the RecipeDataset.\n","\n","        Args:\n","            ingredients (list): List of ingredients for input.\n","            recipes (list): List of recipes for output.\n","            tokenizer (transformers.Tokenizer): Tokenizer for encoding text.\n","            max_length (int): Maximum length of input and output sequences.\n","        \"\"\"\n","        self.ingredients = ingredients\n","        self.recipes = recipes\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns the length of the dataset.\n","        \"\"\"\n","        return len(self.ingredients)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Get an item from the dataset by index.\n","\n","        Args:\n","            idx (int): Index of the item.\n","\n","        Returns:\n","            dict: Dictionary containing the input_ids, attention_mask, and labels.\n","        \"\"\"\n","        # Get the input text and target text\n","        input_text = str(self.ingredients[idx])\n","        target_text = str(self.recipes[idx])\n","\n","        # Tokenize input text\n","        input_tokens = self.tokenizer.encode_plus(\n","            input_text,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Tokenize target text\n","        target_tokens = self.tokenizer.encode(\n","            target_text,\n","            max_length=self.max_length,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Return input_ids, attention_mask, and labels\n","        return {\n","            \"input_ids\": input_tokens.input_ids.flatten(),\n","            \"attention_mask\": input_tokens.attention_mask.flatten(),\n","            \"labels\": target_tokens.flatten()\n","        }"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T19:48:00.281868Z","iopub.status.busy":"2024-04-25T19:48:00.281559Z","iopub.status.idle":"2024-04-25T19:48:05.068425Z","shell.execute_reply":"2024-04-25T19:48:05.067605Z","shell.execute_reply.started":"2024-04-25T19:48:00.281843Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edf27240d0ba4825ab2f4df2465bcd4c","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5d0e9bce3044efca4c484d75072a320","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9425f69e7784c3b9bb53c631be9d409","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2938fd8ae5048e08458a323a96d5e23","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbca1d4a96884388923cd234daaae29e","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Split the dataset into train and validation sets\n","train_inst, val_test_inst, train_resp, val_test_resp = train_test_split(\n","    inst, resp, test_size=0.2, random_state=42\n",")\n","\n","val_inst, test_inst, val_resp, test_resp = train_test_split(\n","    val_test_inst, val_test_resp, test_size=0.5, random_state=42\n",")\n","\n","# Initialize the tokenizer and model\n","tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n","model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n","\n","# Prepare the dataset and dataloaders\n","train_dataset = RecipeDataset(train_inst, train_resp, tokenizer)\n","val_dataset = RecipeDataset(val_inst, val_resp, tokenizer)\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Training the Model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T19:48:05.070148Z","iopub.status.busy":"2024-04-25T19:48:05.069864Z","iopub.status.idle":"2024-04-25T19:48:05.081221Z","shell.execute_reply":"2024-04-25T19:48:05.080112Z","shell.execute_reply.started":"2024-04-25T19:48:05.070123Z"},"trusted":true},"outputs":[],"source":["def train_recipe_model(model, device, train_dataloader, val_dataloader, optimizer, epochs=1):\n","    \"\"\"\n","    Train the recipe generation model.\n","\n","    Args:\n","        model (torch.nn.Module): The recipe generation model.\n","        train_dataloader (DataLoader): Dataloader for training data.\n","        val_dataloader (DataLoader): Dataloader for validation data.\n","        optimizer (torch.optim.Optimizer): Optimizer for training.\n","        epochs (int): Number of epochs for training. Default is 1.\n","    \"\"\"\n","    # Define training parameters\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    # Training loop\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0.0\n","        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        # Validation loop\n","        model.eval()\n","        val_loss = 0.0\n","        with torch.no_grad():\n","            for batch in tqdm(val_dataloader, desc=f\"Validation\"):\n","                input_ids = batch[\"input_ids\"].to(device)\n","                attention_mask = batch[\"attention_mask\"].to(device)\n","                labels = batch[\"labels\"].to(device)\n","\n","                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = outputs.loss\n","\n","                val_loss += loss.item()\n","\n","        # Calculate average training and validation loss\n","        avg_train_loss = train_loss / len(train_dataloader)\n","        avg_val_loss = val_loss / len(val_dataloader)\n","\n","        # Print epoch-wise training and validation loss\n","        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}\")\n","\n","    # Save the trained model\n","    torch.save(model.state_dict(), \"recipe_generation_model.pth\")\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T19:48:05.082700Z","iopub.status.busy":"2024-04-25T19:48:05.082365Z","iopub.status.idle":"2024-04-25T21:55:54.765695Z","shell.execute_reply":"2024-04-25T21:55:54.764795Z","shell.execute_reply.started":"2024-04-25T19:48:05.082668Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/1: 100%|██████████| 20000/20000 [2:03:00<00:00,  2.71it/s]  \n","Validation: 100%|██████████| 2500/2500 [04:47<00:00,  8.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/1, Train Loss: 0.7127893011666835, Val Loss: 0.5979571074783802\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n","epochs = 1\n","model = train_recipe_model(model, device, train_dataloader, val_dataloader, optimizer, epochs=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Saving the model\n","\n","Saving in the Hugging Face format"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:34:23.567343Z","iopub.status.busy":"2024-04-25T22:34:23.566664Z","iopub.status.idle":"2024-04-25T22:34:29.750690Z","shell.execute_reply":"2024-04-25T22:34:29.749802Z","shell.execute_reply.started":"2024-04-25T22:34:23.567312Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n","Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n","Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"]},{"data":{"text/plain":["('Recipe_Generation_BART/tokenizer_config.json',\n"," 'Recipe_Generation_BART/special_tokens_map.json',\n"," 'Recipe_Generation_BART/vocab.json',\n"," 'Recipe_Generation_BART/merges.txt',\n"," 'Recipe_Generation_BART/added_tokens.json')"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# CONFIGURATION\n","# Load the model configuration\n","config = BartConfig.from_pretrained('facebook/bart-base', output_hidden_states=False)\n","# Save the configuration to disk\n","config.save_pretrained('Recipe_Generation_BART')\n","\n","# MODEL\n","# Load your fine-tuned model weights into the predefined BART architecture\n","model = BartForConditionalGeneration(config)\n","state_dict = torch.load('/kaggle/working/recipe_generation_model.pth')\n","model.load_state_dict(state_dict)\n","# Save the model in the Hugging Face format\n","model.save_pretrained('Recipe_Generation_BART')\n","\n","# TOKENIZER\n","# Load and save the tokenizer\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n","tokenizer.save_pretrained('Recipe_Generation_BART')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:34:29.752529Z","iopub.status.busy":"2024-04-25T22:34:29.752236Z","iopub.status.idle":"2024-04-25T22:35:01.266734Z","shell.execute_reply":"2024-04-25T22:35:01.265627Z","shell.execute_reply.started":"2024-04-25T22:34:29.752504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Created zip file /kaggle/working/Recipe_Generation_BART.zip\n"]}],"source":["def zip_directory(folder_path, output_path):\n","    # Create a zip file at the specified output path\n","    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","        # Walk through all the directories and files in the folder_path\n","        for root, dirs, files in os.walk(folder_path):\n","            for file in files:\n","                # Create a proper path for each file to be added\n","                file_path = os.path.join(root, file)\n","                # Create an archive name to store the file in the zip\n","                # This is the path within the zip file\n","                archive_name = os.path.relpath(file_path, os.path.dirname(folder_path))\n","                # Add the file to the zip file with its new archive name\n","                zipf.write(file_path, arcname=archive_name)\n","    print(f\"Created zip file {output_path}\")\n","    \n","# Directory to be zipped\n","directory_to_zip = \"/kaggle/working/Recipe_Generation_BART\"\n","# Output zip file path\n","zip_output_path = \"/kaggle/working/Recipe_Generation_BART.zip\"\n","\n","# Call the function\n","zip_directory(directory_to_zip, zip_output_path)"]},{"cell_type":"markdown","metadata":{},"source":["Downloaded the zip & Manually uploaded to the Hugging Face Platform"]},{"cell_type":"markdown","metadata":{},"source":["Downloading test data for inference"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-04-25T22:55:36.501285Z","iopub.status.busy":"2024-04-25T22:55:36.500290Z","iopub.status.idle":"2024-04-25T22:55:37.006653Z","shell.execute_reply":"2024-04-25T22:55:37.005393Z","shell.execute_reply.started":"2024-04-25T22:55:36.501242Z"},"trusted":true},"outputs":[],"source":["# Saving for later inference\n","test_data = {\"Instruction\": test_inst, \"Response\": test_resp}\n","\n","# Create DataFrame\n","test_df = pd.DataFrame(test_data)\n","\n","# Save DataFrame as CSV\n","test_df.to_csv(\"test_data.csv\", index=False)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4846676,"sourceId":8185286,"sourceType":"datasetVersion"},{"datasetId":4862482,"sourceId":8206330,"sourceType":"datasetVersion"},{"datasetId":4873907,"sourceId":8221288,"sourceType":"datasetVersion"},{"datasetId":4880554,"sourceId":8230034,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
